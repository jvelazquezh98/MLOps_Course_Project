# Gu√≠a de Uso de Scripts de Evaluaci√≥n y Visualizaci√≥n

Este documento explica c√≥mo usar los nuevos scripts creados para completar los puntos faltantes del proyecto.

## üìä 1. Visualizaci√≥n de Resultados (`src/visualization/plot_results.py`)

### Descripci√≥n
Genera visualizaciones comprehensivas de los resultados de modelos ML, incluyendo:
- Matrices de confusi√≥n
- Curvas ROC
- Curvas Precision-Recall
- Distribuciones de predicciones
- Importancia de features

### Uso B√°sico

```bash
# Generar todas las visualizaciones para un modelo
python -m src.visualization.plot_results generate-all-plots \
  --predictions-path data/processed/predictions.csv \
  --model-name "random_forest" \
  --output-dir reports/figures
```

### Ejemplos

```bash
# Con modo debug
python -m src.visualization.plot_results generate-all-plots \
  --predictions-path data/processed/predictions.csv \
  --model-name "xgboost" \
  --output-dir reports/figures/xgb \
  --debug

# Para modelo espec√≠fico
python -m src.visualization.plot_results generate-all-plots \
  --predictions-path data/processed/rf_predictions.csv \
  --model-name "rf_v1" \
  --output-dir reports/figures/rf_v1
```

### Salidas Generadas
- `confusion_matrix_{model_name}.png` - Matriz de confusi√≥n
- `roc_curve_{model_name}.png` - Curva ROC con AUC
- `precision_recall_{model_name}.png` - Curva Precision-Recall
- `prediction_distribution_{model_name}.png` - Distribuci√≥n de probabilidades

---

## üîç 2. Comparaci√≥n de Modelos (`src/evaluation/compare_models.py`)

### Descripci√≥n
Compara autom√°ticamente m√∫ltiples modelos ML y genera:
- Tabla comparativa de m√©tricas
- Gr√°ficos de barras comparativos
- Gr√°ficos radar
- Reporte de texto con rankings

### Uso B√°sico

```bash
# Comparar todos los modelos en el directorio de m√©tricas
python -m src.evaluation.compare_models compare \
  --metrics-dir reports/metrics \
  --output-dir reports/comparison
```

### Ejemplos

```bash
# Con modo debug
python -m src.evaluation.compare_models compare \
  --metrics-dir reports/metrics \
  --output-dir reports/comparison \
  --debug

# Directorio personalizado
python -m src.evaluation.compare_models compare \
  --metrics-dir models/experiment_metrics \
  --output-dir reports/model_comparison_2024
```

### Salidas Generadas
- `model_comparison.csv` - Tabla CSV con todas las m√©tricas
- `metrics_comparison_bar.png` - Gr√°fico de barras comparativo
- `metrics_comparison_radar.png` - Gr√°fico radar comparativo
- `comparison_report.txt` - Reporte detallado en texto

### Formato de Entrada
Los archivos JSON de m√©tricas deben tener esta estructura:

```json
{
  "model": "RandomForest",
  "metrics": {
    "accuracy": 0.85,
    "precision": 0.82,
    "recall": 0.88,
    "f1": 0.85,
    "AUC_test_final": 0.90
  }
}
```

---

## üìù 3. Documentaci√≥n de Notebooks (`src/documentation/notebook_documenter.py`)

### Descripci√≥n
Agrega documentaci√≥n inline detallada a notebooks Jupyter de forma automatizada.

### Comandos Disponibles

#### 3.1 Agregar Encabezado

```bash
python -m src.documentation.notebook_documenter add-header \
  notebooks/01_eda.ipynb \
  --title "An√°lisis Exploratorio de Datos" \
  --author "Tu Nombre" \
  --objective "Explorar y entender el dataset" \
  --description "An√°lisis detallado de distribuciones y correlaciones"
```

#### 3.2 Generar Plantilla de Documentaci√≥n

```bash
# Analiza el notebook y genera una plantilla JSON
python -m src.documentation.notebook_documenter generate-template \
  notebooks/01_eda.ipynb \
  --output-path notebooks/01_eda_doc_template.json
```

Esto genera un archivo JSON que puedes editar:

```json
{
  "notebook_name": "01_eda",
  "sections": [
    {
      "number": 1,
      "title": "Carga de Datos",
      "objective": "[COMPLETAR: Objetivo de esta secci√≥n]",
      "description": "[COMPLETAR: Descripci√≥n detallada]",
      "num_cells": 3
    }
  ]
}
```

#### 3.3 Aplicar Documentaci√≥n

```bash
# Aplica la plantilla editada al notebook
python -m src.documentation.notebook_documenter apply-documentation \
  notebooks/01_eda.ipynb \
  notebooks/01_eda_doc_template.json \
  --output-path notebooks/01_eda_documented.ipynb
```

### Ejemplos Completos

```bash
# Workflow completo de documentaci√≥n
# 1. Generar plantilla
python -m src.documentation.notebook_documenter generate-template \
  notebooks/02_preprocessing.ipynb

# 2. Editar el archivo JSON generado manualmente

# 3. Aplicar documentaci√≥n
python -m src.documentation.notebook_documenter apply-documentation \
  notebooks/02_preprocessing.ipynb \
  notebooks/02_preprocessing_doc_template.json

# 4. Agregar encabezado final
python -m src.documentation.notebook_documenter add-header \
  notebooks/02_preprocessing_documented.ipynb \
  --title "Preprocesamiento de Datos" \
  --author "Data Team" \
  --objective "Limpiar y transformar datos para modelado"
```

---

## üöÄ Workflow Completo Recomendado

### Paso 1: Entrenar Modelos y Generar M√©tricas
```bash
# Ya tienes esto con tu script de entrenamiento
python -m src.modeling.train --data-path data/processed/features.csv
```

### Paso 2: Generar Visualizaciones
```bash
# Para cada modelo entrenado
python -m src.visualization.plot_results generate-all-plots \
  --predictions-path data/processed/predictions.csv \
  --model-name "rf_model" \
  --output-dir reports/figures
```

### Paso 3: Comparar Modelos
```bash
# Comparar todos los modelos
python -m src.evaluation.compare_models compare \
  --metrics-dir reports/metrics \
  --output-dir reports/comparison
```

### Paso 4: Documentar Notebooks
```bash
# Para cada notebook
python -m src.documentation.notebook_documenter add-header \
  notebooks/03_training.ipynb \
  --title "Entrenamiento de Modelos" \
  --author "ML Team" \
  --objective "Entrenar y evaluar modelos de clasificaci√≥n"
```

---

## üìã Requisitos

Aseg√∫rate de tener instaladas las dependencias:

```bash
pip install pandas numpy matplotlib seaborn scikit-learn typer loguru tabulate
```

O con uv:

```bash
uv pip install pandas numpy matplotlib seaborn scikit-learn typer loguru tabulate
```

---

## üéØ Puntos de la Matriz Completados

Con estos scripts completas:

‚úÖ **Resultados (15 pts)**: Visualizaciones comprehensivas y presentaci√≥n clara de m√©tricas

‚úÖ **M√©todos y t√©cnicas (20 pts)**: Comparaci√≥n automatizada justifica selecci√≥n de modelos

‚úÖ **Documentaci√≥n inline**: Notebooks con documentaci√≥n detallada y estructurada

---

## üí° Tips

1. **Ejecuta los scripts despu√©s de cada entrenamiento** para mantener visualizaciones actualizadas

2. **Usa el modo --debug** cuando algo no funcione para ver logs detallados

3. **Personaliza las plantillas** en `DOCUMENTATION_TEMPLATES` seg√∫n tus necesidades

4. **Integra con MLflow** para tracking autom√°tico de visualizaciones

---

## üêõ Troubleshooting

### Error: "No se encontraron m√©tricas"
- Verifica que los archivos JSON est√©n en `reports/metrics/`
- Revisa que tengan la estructura correcta

### Error: "Columna 'popular' no encontrada"
- Aseg√∫rate de que tu CSV de predicciones incluya la columna target
- O modifica el script para usar otra columna

### Gr√°ficos no se generan
- Verifica que matplotlib est√© instalado correctamente
- En servidores sin GUI, usa backend 'Agg': `matplotlib.use('Agg')`

---

¬øNecesitas ayuda? Revisa los logs con `--debug` o consulta la documentaci√≥n inline de cada script.
